{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00526c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha256 as sha\n",
    "from json import dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8af7b112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunk_id(content: str) -> str:\n",
    "    \"\"\"Genera un ID único para el chunk usando un hash SHA256.\"\"\"\n",
    "    return sha((content).encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e05abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_chunks(input_filepath: str, output_filepath: str):\n",
    "    allChunks       = {}\n",
    "    processed_chunk = None\n",
    "    content_data    = None\n",
    "    #\n",
    "    with open(input_filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines() # Leer todas las líneas de una vez para un mejor manejo de bloques\n",
    "    for line in lines:\n",
    "        #\n",
    "        # Procesamiento de campos\n",
    "        #\n",
    "        if \"ID_AUTO\" in line:\n",
    "            continue\n",
    "        elif \"---CHUNK_START---\" in line:\n",
    "            processed_chunk = dict()\n",
    "        elif (processed_chunk != None) and \"source_file:\" in line:\n",
    "            thisData = \"\".join(line.split(\":\")[1]).replace('\\n','').replace('\\t','').lstrip()\n",
    "            processed_chunk[\"source_file\"] = thisData\n",
    "        elif (processed_chunk != None) and \"section_heading:\" in line:\n",
    "            thisData = \"\".join(line.split(\":\")[1]).replace('\\n','').replace('\\t','').lstrip()\n",
    "            processed_chunk[\"section_heading\"] = thisData\n",
    "        elif (processed_chunk != None) and \"page_number:\" in line:\n",
    "            thisData = \"\".join(line.split(\":\")[1]).replace('\\n','').replace('\\t','').lstrip()\n",
    "            processed_chunk[\"page_number\"] = thisData\n",
    "        elif (processed_chunk != None) and \"language:\" in line:\n",
    "            thisData = \"\".join(line.split(\":\")[1]).replace('\\n','').replace('\\t','').lstrip()\n",
    "            processed_chunk[\"language\"] = thisData\n",
    "        elif (processed_chunk != None) and \"group:\" in line:\n",
    "            thisData = \"\".join(line.split(\":\")[1]).replace('\\n','').replace('\\t','').lstrip()\n",
    "            processed_chunk[\"group\"] = thisData\n",
    "        elif (processed_chunk != None) and \"tags:\" in line:\n",
    "            thisData = [x.lstrip() for x in \"\".join(line.split(\":\")[1]).replace('\\n','').replace('\\t','').lstrip().split(\",\")]\n",
    "            processed_chunk[\"tags\"] = thisData\n",
    "        #\n",
    "        # Se inicia procesamiento de contenido multiliean?\n",
    "        #\n",
    "        elif \"content_start\" in line:\n",
    "            content_data = list()\n",
    "        #\n",
    "        elif content_data != None and \"content_end\" not in line:\n",
    "            # Aquí, solo añade la línea, sin hacer escapes manuales\n",
    "            content_data.append(line) # No .replace('\"', \"&quot; \").replace(\"'\", \"&#39; \")\n",
    "            # content_data.append('\\n') # Esto tampoco es necesario si la línea ya viene con un salto de línea y lo quieres conservar\n",
    "        elif \"content_end\" in line:\n",
    "            # Reemplaza 'content_data = \",\".join([x.lstrip() for x in content_data])'\n",
    "            # Simplemente une las líneas sin comas adicionales, y elimina los saltos de línea extra si los hay al final.\n",
    "            processed_chunk[\"content\"] = \"\".join(content_data).strip() \n",
    "            # Asegúrate de que no haya comas extra o saltos de línea dobles al unir.\n",
    "            # Puedes probar: processed_chunk[\"content\"] = \"\\n\".join([l.rstrip('\\n') for l in content_data]).strip()\n",
    "            # Esto eliminará saltos de línea al final de cada línea antes de unirlas con '\\n'\n",
    "            \n",
    "            content_data = None\n",
    "        #\n",
    "        # Si se detecta finalizacion de chunk\n",
    "        #\n",
    "        elif \"---CHUNK_END---\" in line:\n",
    "            #allData = pd.concat([allData, pd.DataFrame(processed_chunk)], ignore_index=True)\n",
    "            allChunks[generate_chunk_id(str(processed_chunk))] = processed_chunk\n",
    "    #\n",
    "    # JSON outoput\n",
    "    #\n",
    "        # Esto es crucial: usar json.dumps() con el diccionario Python directamente.\n",
    "    # NO: f.write(str(allChunks).replace(\"'\", '\"') + '\\n')\n",
    "    # SÍ:\n",
    "    with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "        # Itera sobre los chunks en el diccionario allChunks\n",
    "        f.write(dumps(allChunks, ensure_ascii=False) + '\\n') \n",
    "        '''\n",
    "        for chunk_data in allChunks.items():\n",
    "            # json.dumps() se encarga automáticamente de escapar las comillas dobles internas\n",
    "            # y de manejar los caracteres Unicode.\n",
    "            print(chunk_data)\n",
    "            f.write(dumps(chunk_data, ensure_ascii=False) + '\\n') '''\n",
    "    print(f\"Chunks procesados y guardados en {output_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "effaaac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks procesados y guardados en processed_chunks_safe.json\n"
     ]
    }
   ],
   "source": [
    "parse_chunks(\"sourceOfTrue.txt\", \"processed_chunks_safe.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
